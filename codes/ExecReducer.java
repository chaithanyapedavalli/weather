package org.apache.hadoop.hive.ql.exec.mr;
//Java code for Hive query for custom MapReduce Jobs
//Generated by HIVE compiler and packaged as part of job.jar that is passed to Hadoop to execute MapReduce job

/*
**	Packages imported:
**	Apache, java, slf4j, util
*/

import org.slf4j.LoggerFactory;
import java.io.IOException;
import org.apache.hadoop.util.StringUtils;
import org.apache.hadoop.hive.serde2.SerDeException;
import org.apache.hadoop.hive.ql.metadata.HiveException;
import org.apache.hadoop.io.Writable;
import java.util.Iterator;
import org.apache.hadoop.hive.ql.plan.ReduceWork;
import org.apache.hadoop.hive.ql.exec.MapredContext;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory;
import java.util.Properties;
import org.apache.hadoop.hive.serde2.SerDeUtils;
import org.apache.hadoop.util.ReflectionUtils;
import org.apache.hadoop.hive.ql.plan.OperatorDesc;
import org.apache.hadoop.conf.Configuration;
import java.util.Arrays;
import java.net.URLClassLoader;
import java.util.ArrayList;
import org.apache.hadoop.hive.ql.exec.Utilities;
import org.apache.hadoop.io.BytesWritable;
import org.apache.hadoop.hive.serde2.objectinspector.ObjectInspector;
import org.apache.hadoop.hive.ql.plan.TableDesc;
import org.apache.hadoop.mapred.Reporter;
import org.apache.hadoop.hive.ql.exec.Operator;
import org.apache.hadoop.mapred.OutputCollector;
import org.apache.hadoop.mapred.JobConf;
import java.util.List;
import org.apache.hadoop.hive.serde2.Deserializer;
import org.slf4j.Logger;
import org.apache.hadoop.mapred.Reducer;
import org.apache.hadoop.mapred.MapReduceBase;

public class ExecReducer extends MapReduceBase implements Reducer
{
    private static final Logger LOG;
    private static final boolean isInfoEnabled;
    private static final boolean isTraceEnabled;
    private static final String PLAN_KEY = "__REDUCE_PLAN__";
    private final Deserializer[] inputValueDeserializer;
    private final Object[] valueObject;
    private final List<Object> row;
    private Deserializer inputKeyDeserializer;
    private JobConf jc;
    private OutputCollector<?, ?> oc;
    private Operator<?> reducer;
    private Reporter rp;
    private boolean abort;
    private boolean isTagged;
    private TableDesc keyTableDesc;
    private TableDesc[] valueTableDesc;
    private ObjectInspector[] rowObjectInspector;
    private transient Object keyObject;
    private transient BytesWritable groupKey;
    
    public ExecReducer() {
        this.inputValueDeserializer = new Deserializer[127];
        this.valueObject = new Object[127];
        this.row = new ArrayList<Object>(Utilities.reduceFieldNameList.size());
        this.abort = false;
        this.isTagged = false;
    }
    /*
	** Configure Reduce Job
	*/
    public void configure(final JobConf job) {
		//Job is an object for which its configuration
		//Remember, Hive determines the job plan before its executed in HDFS
        this.rowObjectInspector = new ObjectInspector[127];
        final ObjectInspector[] valueObjectInspector = new ObjectInspector[127];
        if (ExecReducer.isInfoEnabled) {
            try {
                ExecReducer.LOG.info("conf classpath = " + Arrays.asList(((URLClassLoader)job.getClassLoader()).getURLs()));
                ExecReducer.LOG.info("thread classpath = " + Arrays.asList(((URLClassLoader)Thread.currentThread().getContextClassLoader()).getURLs()));
            }
            catch (Exception e) {
                ExecReducer.LOG.info("cannot get classpath: " + e.getMessage());
            }
        }
        this.jc = job;
		/*
		** Fetch key value pairs and configure reduce job to apply them
		*/
        final ReduceWork gWork = Utilities.getReduceWork((Configuration)job);
        (this.reducer = gWork.getReducer()).setParentOperators(null);
        this.isTagged = gWork.getNeedsTagging();
        try {
            this.keyTableDesc = gWork.getKeyDesc();
            SerDeUtils.initializeSerDe(this.inputKeyDeserializer = (Deserializer)ReflectionUtils.newInstance((Class)this.keyTableDesc.getDeserializerClass(), (Configuration)null), null, this.keyTableDesc.getProperties(), null);
            final ObjectInspector keyObjectInspector = this.inputKeyDeserializer.getObjectInspector();
            this.valueTableDesc = new TableDesc[gWork.getTagToValueDesc().size()];
            for (int tag = 0; tag < gWork.getTagToValueDesc().size(); ++tag) {
                this.valueTableDesc[tag] = gWork.getTagToValueDesc().get(tag);
                SerDeUtils.initializeSerDe(this.inputValueDeserializer[tag] = (Deserializer)ReflectionUtils.newInstance((Class)this.valueTableDesc[tag].getDeserializerClass(), (Configuration)null), null, this.valueTableDesc[tag].getProperties(), null);
                valueObjectInspector[tag] = this.inputValueDeserializer[tag].getObjectInspector();
                final ArrayList<ObjectInspector> ois = new ArrayList<ObjectInspector>();
                ois.add(keyObjectInspector);
                ois.add(valueObjectInspector[tag]);
                this.rowObjectInspector[tag] = ObjectInspectorFactory.getStandardStructObjectInspector(Utilities.reduceFieldNameList, ois);
            }
        }
        catch (Exception e2) {
            throw new RuntimeException(e2);
        }
        MapredContext.init(false, new JobConf((Configuration)this.jc));
        try {
            ExecReducer.LOG.info(this.reducer.dump(0));
            this.reducer.initialize((Configuration)this.jc, this.rowObjectInspector);
        }
        catch (Throwable e3) {
            this.abort = true;
            if (e3 instanceof OutOfMemoryError) {
                throw (OutOfMemoryError)e3;
            }
            throw new RuntimeException("Reduce operator initialization failed", e3);
        }
    }
	//Define Reducr class
	/*
	** reduce function that takes input of Map Output i.e. Key and Value pairs and perform reduce job on them
	** provides updates via slf4j logger
	*/
    
    public void reduce(final Object key, final Iterator values, final OutputCollector output, final Reporter reporter) throws IOException {
        if (this.reducer.getDone()) {
            return;
        }
        if (this.oc == null) {
            this.oc = (OutputCollector<?, ?>)output;
            this.rp = reporter;
            this.reducer.setReporter(this.rp);
            MapredContext.get().setReporter(reporter);
        }
        try {
            final BytesWritable keyWritable = (BytesWritable)key;
            byte tag = 0;
            if (this.isTagged) {
                final int size = keyWritable.getSize() - 1;
                tag = keyWritable.get()[size];
                keyWritable.setSize(size);
            }
            if (!keyWritable.equals((Object)this.groupKey)) {
                if (this.groupKey == null) {
                    this.groupKey = new BytesWritable();
                }
                else {
                    if (ExecReducer.isTraceEnabled) {
                        ExecReducer.LOG.trace("End Group");
                    }
                    this.reducer.endGroup();
                }
                try {
                    this.keyObject = this.inputKeyDeserializer.deserialize((Writable)keyWritable);
                }
                catch (Exception e) {
                    throw new HiveException("Hive Runtime Error: Unable to deserialize reduce input key from " + Utilities.formatBinaryString(keyWritable.get(), 0, keyWritable.getSize()) + " with properties " + this.keyTableDesc.getProperties(), e);
                }
                this.groupKey.set(keyWritable.get(), 0, keyWritable.getSize());
                if (ExecReducer.isTraceEnabled) {
                    ExecReducer.LOG.trace("Start Group");
                }
                this.reducer.startGroup();
                this.reducer.setGroupKeyObject(this.keyObject);
            }
            while (values.hasNext()) {
                final BytesWritable valueWritable = values.next();
                try {
                    this.valueObject[tag] = this.inputValueDeserializer[tag].deserialize((Writable)valueWritable);
                }
                catch (SerDeException e2) {
                    throw new HiveException("Hive Runtime Error: Unable to deserialize reduce input value (tag=" + tag + ") from " + Utilities.formatBinaryString(valueWritable.get(), 0, valueWritable.getSize()) + " with properties " + this.valueTableDesc[tag].getProperties(), e2);
                }
                this.row.clear();
                this.row.add(this.keyObject);
                this.row.add(this.valueObject[tag]);
                try {
                    this.reducer.process(this.row, tag);
                }
                catch (Exception e4) {
                    String rowString = null;
                    try {
                        rowString = SerDeUtils.getJSONString(this.row, this.rowObjectInspector[tag]);
                    }
                    catch (Exception e3) {
                        rowString = "[Error getting row data with exception " + StringUtils.stringifyException((Throwable)e3) + " ]";
                    }
                    throw new HiveException("Hive Runtime Error while processing row (tag=" + tag + ") " + rowString, e4);
                }
            }
        }
        catch (Throwable e5) {
            this.abort = true;
            if (e5 instanceof OutOfMemoryError) {
                throw (OutOfMemoryError)e5;
            }
            ExecReducer.LOG.error(StringUtils.stringifyException(e5));
            throw new RuntimeException(e5);
        }
    }
    
    public void close() {
        if (this.oc == null && ExecReducer.isTraceEnabled) {
            ExecReducer.LOG.trace("Close called without any rows processed");
        }
        try {
            if (this.groupKey != null) {
                if (ExecReducer.isTraceEnabled) {
                    ExecReducer.LOG.trace("End Group");
                }
                this.reducer.endGroup();
            }
            this.reducer.close(this.abort);
            final ExecMapper.ReportStats rps = new ExecMapper.ReportStats(this.rp, (Configuration)this.jc);
            this.reducer.preorderMap(rps);
        }
        catch (Exception e) {
            if (!this.abort) {
                ExecReducer.LOG.error("Hit error while closing operators - failing tree");
                throw new RuntimeException("Hive Runtime Error while closing operators: " + e.getMessage(), e);
            }
        }
        finally {
            MapredContext.close();
            Utilities.clearWorkMap((Configuration)this.jc);
        }
    }
    
    static {
        LOG = LoggerFactory.getLogger("ExecReducer");
        isInfoEnabled = ExecReducer.LOG.isInfoEnabled();
        isTraceEnabled = ExecReducer.LOG.isTraceEnabled();
    }
}
